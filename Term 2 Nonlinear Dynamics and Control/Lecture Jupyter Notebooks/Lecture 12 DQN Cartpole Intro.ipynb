{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d4bb31d",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "\n",
    "Пересчёт Q-функции выполняется по правилу\n",
    "### $Q_{k+1}(s, a) = (1-\\alpha) \\cdot Q_{k}(s, a) + \\alpha \\cdot \\hat{Q}(s,a) =  Q_{k}(s, a) + \\alpha \\cdot (\\hat{Q}(s,a) - Q_{k}(s, a))$\n",
    "\n",
    "\n",
    "## Q-learning с использованием нейросети\n",
    "Теперь $Q$-функция зависит от параметров аппроксимирующей её нейросети $\\theta$, то есть на каждой итерации\n",
    "\n",
    "### $Q_{\\theta}(s, a) \\leftarrow Q_{{\\theta}}(s, a) + \\alpha \\cdot (r(s,a) + \\gamma \\cdot \\max_{a'}{Q_{\\theta}(s',a')} - Q_{\\theta}(s, a))$\n",
    "\n",
    "Таким образом, решается следующая задача регрессии:\n",
    "\n",
    "Пусть у нас есть текущая версия $Q_{\\theta_k}k(s, a)$, и мы хотим проделать шаг метода простой итерации для решения уравнения Беллмана:\n",
    "### $Q^*(s,a) =  r(s,a) + \\gamma E_{s'}[\\max_{a'}{Q^*(s',a')}]$\n",
    "\n",
    "\n",
    "- входом является пара $s, a$\n",
    "\n",
    "- искомым значением на паре $s, a$ является правая часть уравнения оптимальности Беллмана, то есть\n",
    "### $f(s, a) \\leftarrow r(s,a) + \\gamma E_{s'}[\\max_{a'}{Q_{\\theta_k}(s',a')}]$\n",
    "\n",
    "- наблюдаемым значением является\n",
    "### $y(s, a) \\leftarrow r(s,a) + \\gamma \\max_{a'}{Q_{\\theta_k}(s',a')},$\n",
    "где $s' \\leftarrow p(s'|s,a).$\n",
    "\n",
    "- функция потерь определена как\n",
    "### $Loss(y, \\hat{y}) = \\frac{1}{2}(y - \\hat{y})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "843ebe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/gsurma/cartpole\n",
    "    \n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import adam_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86909f32",
   "metadata": {},
   "source": [
    "### Реализация 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "186c5878",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"CartPole-v1\"\n",
    "\n",
    "GAMMA = 0.95\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "MEMORY_SIZE = 1000000\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.01\n",
    "EXPLORATION_DECAY = 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1a50a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNSolver:\n",
    "\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "\n",
    "        self.action_space = action_space\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_shape=(observation_space,), activation=\"relu\"))\n",
    "        self.model.add(Dense(24, activation=\"relu\"))\n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))\n",
    "        self.model.compile(loss=\"mse\", optimizer=adam_v2.Adam(learning_rate=LEARNING_RATE))\n",
    "\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "        \n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return random.randrange(self.action_space)\n",
    "        \n",
    "        q_values = self.model.predict(state)\n",
    "        \n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    \n",
    "    def experience_replay(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.memory, BATCH_SIZE)\n",
    "        for state, action, reward, state_next, terminal in batch:\n",
    "            q_update = reward\n",
    "            \n",
    "            if not terminal:\n",
    "                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))\n",
    "                \n",
    "            q_values = self.model.predict(state)\n",
    "            q_values[0][action] = q_update\n",
    "            self.model.fit(state, q_values, verbose=0)\n",
    "            \n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de2ce3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartpole():\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    observation_space = env.observation_space.shape[0]\n",
    "    action_space = env.action_space.n\n",
    "    dqn_solver = DQNSolver(observation_space, action_space)\n",
    "    \n",
    "    for i in range(100):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, observation_space])\n",
    "        episode_len = 0\n",
    "        \n",
    "        while True:\n",
    "            action = dqn_solver.act(state)\n",
    "            state_next, reward, terminal_state, info = env.step(action)\n",
    "            reward = reward if not terminal_state else -reward\n",
    "            state_next = np.reshape(state_next, [1, observation_space])\n",
    "            dqn_solver.remember(state, action, reward, state_next, terminal_state)\n",
    "            dqn_solver.experience_replay()\n",
    "            state = state_next\n",
    "            episode_len += 1\n",
    "            \n",
    "            if terminal_state:\n",
    "                print(i, '-', episode_len)\n",
    "                break\n",
    "    \n",
    "    terminal_state = False\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, observation_space])               \n",
    "    \n",
    "    while True:\n",
    "        #env.render()\n",
    "        action = dqn_solver.act(state)\n",
    "        state_next, reward, terminal, info = env.step(action)\n",
    "        state = np.reshape(state_next, [1, observation_space])\n",
    "        print(state)\n",
    "          \n",
    "        if terminal:\n",
    "           \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4b12567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - 15\n",
      "1 - 12\n",
      "2 - 10\n",
      "3 - 17\n",
      "4 - 11\n",
      "5 - 15\n",
      "6 - 15\n",
      "7 - 13\n",
      "8 - 10\n",
      "9 - 11\n",
      "10 - 19\n",
      "11 - 12\n",
      "12 - 10\n",
      "13 - 23\n",
      "14 - 10\n",
      "15 - 11\n",
      "16 - 18\n",
      "17 - 17\n",
      "18 - 37\n",
      "19 - 25\n",
      "20 - 33\n",
      "21 - 42\n",
      "22 - 52\n",
      "23 - 30\n",
      "24 - 58\n",
      "25 - 81\n",
      "26 - 48\n",
      "27 - 73\n",
      "28 - 86\n",
      "29 - 73\n",
      "30 - 95\n",
      "31 - 112\n",
      "32 - 124\n",
      "33 - 125\n",
      "34 - 92\n",
      "35 - 117\n",
      "36 - 119\n",
      "37 - 117\n",
      "38 - 140\n",
      "39 - 202\n",
      "40 - 132\n",
      "41 - 130\n",
      "42 - 148\n",
      "43 - 168\n",
      "44 - 177\n",
      "45 - 139\n",
      "46 - 159\n",
      "47 - 135\n",
      "48 - 240\n",
      "49 - 182\n",
      "50 - 180\n",
      "51 - 150\n",
      "52 - 175\n",
      "53 - 151\n",
      "54 - 172\n",
      "55 - 180\n",
      "56 - 167\n",
      "57 - 175\n",
      "58 - 159\n",
      "59 - 185\n",
      "60 - 164\n",
      "61 - 144\n",
      "62 - 142\n",
      "63 - 148\n",
      "64 - 155\n",
      "65 - 166\n",
      "66 - 140\n",
      "67 - 153\n",
      "68 - 192\n",
      "69 - 137\n",
      "70 - 186\n",
      "71 - 169\n",
      "72 - 155\n",
      "73 - 151\n",
      "74 - 140\n",
      "75 - 148\n",
      "76 - 133\n",
      "77 - 134\n",
      "78 - 144\n",
      "79 - 150\n",
      "80 - 147\n",
      "81 - 157\n",
      "82 - 182\n",
      "83 - 139\n",
      "84 - 140\n",
      "85 - 135\n",
      "86 - 158\n",
      "87 - 176\n",
      "88 - 170\n",
      "89 - 169\n",
      "90 - 158\n",
      "91 - 158\n",
      "92 - 144\n",
      "93 - 134\n",
      "94 - 177\n",
      "95 - 141\n",
      "96 - 194\n",
      "97 - 169\n",
      "98 - 186\n",
      "99 - 129\n",
      "[[ 0.00334804 -0.2178733  -0.04252609  0.26176864]]\n",
      "[[-0.00100943 -0.02217093 -0.03729071 -0.04401844]]\n",
      "[[-0.00145285 -0.21673885 -0.03817108  0.23666961]]\n",
      "[[-0.00578762 -0.02109293 -0.03343769 -0.06780506]]\n",
      "[[-0.00620948 -0.21571994 -0.03479379  0.2141434 ]]\n",
      "[[-0.01052388 -0.02011826 -0.03051092 -0.08930884]]\n",
      "[[-0.01092624 -0.2147899  -0.0322971   0.19359389]]\n",
      "[[-0.01522204 -0.01922118 -0.02842522 -0.1090999 ]]\n",
      "[[-0.01560647 -0.21392451 -0.03060722  0.17448123]]\n",
      "[[-0.01988496 -0.0183782  -0.0271176  -0.12769799]]\n",
      "[[-0.02025252 -0.21310139 -0.02967156  0.15630782]]\n",
      "[[-0.02451455 -0.01756745 -0.0265454  -0.14558615]]\n",
      "[[-0.0248659  -0.21229939 -0.02945712  0.13860543]]\n",
      "[[-0.02911188 -0.01676819 -0.02668501 -0.16322328]]\n",
      "[[-0.02944725 -0.21149817 -0.02994948  0.12092324]]\n",
      "[[-0.03367721 -0.4061785  -0.02753102  0.40400895]]\n",
      "[[-0.04180078 -0.21067715 -0.01945084  0.10277495]]\n",
      "[[-0.04601433 -0.01528193 -0.01739534 -0.19598064]]\n",
      "[[-0.04631997 -0.2101508  -0.02131495  0.09116447]]\n",
      "[[-0.05052298 -0.01472991 -0.01949166 -0.20816638]]\n",
      "[[-0.05081758 -0.2095678  -0.02365499  0.07830475]]\n",
      "[[-0.05500893 -0.4043428  -0.02208889  0.3634316 ]]\n",
      "[[-0.06309579 -0.20891398 -0.01482026  0.06386628]]\n",
      "[[-0.06727407 -0.01358272 -0.01354294 -0.23345543]]\n",
      "[[-0.06754573 -0.20850858 -0.01821204  0.05492503]]\n",
      "[[-0.0717159  -0.40336472 -0.01711354  0.34180674]]\n",
      "[[-0.07978319 -0.20800352 -0.01027741  0.04377671]]\n",
      "[[-0.08394326 -0.01273572 -0.00940188 -0.25213104]]\n",
      "[[-0.08419798 -0.20772216 -0.0144445   0.03757159]]\n",
      "[[-0.08835242 -0.01239609 -0.01369306 -0.25963348]]\n",
      "[[-0.08860034 -0.20731992 -0.01888573  0.0286992 ]]\n",
      "[[-0.09274674 -0.402166   -0.01831175  0.31536415]]\n",
      "[[-0.10079006 -0.20678806 -0.01200447  0.01696302]]\n",
      "[[-0.10492582 -0.01149603 -0.01166521 -0.27948314]]\n",
      "[[-0.10515574 -0.20644966 -0.01725487  0.00949788]]\n",
      "[[-0.10928474 -0.40131995 -0.01706491  0.29668713]]\n",
      "[[-0.11731113 -0.20595893 -0.01113117 -0.00132855]]\n",
      "[[-0.12143032 -0.01067913 -0.01115774 -0.2975026 ]]\n",
      "[[-0.12164389 -0.20564027 -0.01710779 -0.00835942]]\n",
      "[[-0.1257567  -0.40051275 -0.01727498  0.27887708]]\n",
      "[[-0.13376695 -0.20514867 -0.01169744 -0.01920391]]\n",
      "[[-0.13786992 -0.00986093 -0.01208152 -0.31555444]]\n",
      "[[-0.13806714 -0.20480873 -0.01839261 -0.02670597]]\n",
      "[[-0.14216332 -0.39966214 -0.01892673  0.26011762]]\n",
      "[[-0.15015656 -0.20427519 -0.01372437 -0.03847445]]\n",
      "[[-0.15424207 -0.00895915 -0.01449386 -0.33545578]]\n",
      "[[-0.15442125 -0.20387186 -0.02120298 -0.04737845]]\n",
      "[[-0.15849869 -0.39868346 -0.02215055  0.23854005]]\n",
      "[[-0.16647236 -0.2032522  -0.01737975 -0.06104667]]\n",
      "[[-0.1705374  -0.3981207  -0.01860068  0.22610256]]\n",
      "[[-0.17849982 -0.20273793 -0.01407863 -0.07238919]]\n",
      "[[-0.18255457 -0.00741699 -0.01552641 -0.36948052]]\n",
      "[[-0.18270291 -0.20231494 -0.02291602 -0.08173343]]\n",
      "[[-0.18674922 -0.397101   -0.02455069  0.20363225]]\n",
      "[[-0.19469123 -0.20163673 -0.02047805 -0.096693  ]]\n",
      "[[-0.19872397 -0.39645928 -0.02241191  0.18945944]]\n",
      "[[-0.20665315 -0.201024   -0.01862272 -0.11020844]]\n",
      "[[-0.21067363 -0.3958742  -0.02082689  0.17654145]]\n",
      "[[-0.21859112 -0.2004605  -0.01729606 -0.12263823]]\n",
      "[[-0.22260033 -0.39533043 -0.01974882  0.16453817]]\n",
      "[[-0.23050694 -0.19993141 -0.01645806 -0.13430896]]\n",
      "[[-0.23450556 -0.3948138  -0.01914424  0.15313658]]\n",
      "[[-0.24240184 -0.19942304 -0.01608151 -0.14552392]]\n",
      "[[-0.2463903  -0.39431104 -0.01899198  0.14204253]]\n",
      "[[-0.2542765  -0.19892232 -0.01615113 -0.15657102]]\n",
      "[[-0.25825498 -0.39380935 -0.01928255  0.13097318]]\n",
      "[[-0.26613116 -0.19841656 -0.01666309 -0.16773024]]\n",
      "[[-0.2700995  -0.3932961  -0.0200177   0.11964974]]\n",
      "[[-0.2779654  -0.19789313 -0.0176247  -0.17928077]]\n",
      "[[-0.28192326 -0.3927585  -0.02121032  0.10779048]]\n",
      "[[-0.28977844 -0.19733912 -0.01905451 -0.19150801]]\n",
      "[[-0.29372522 -0.39218336 -0.02288467  0.09510362]]\n",
      "[[-0.3015689  -0.19674103 -0.0209826  -0.20471063]]\n",
      "[[-0.30550373 -0.39155674 -0.02507681  0.08128016]]\n",
      "[[-0.31333485 -0.19608444 -0.0234512  -0.2192079 ]]\n",
      "[[-0.31725654 -0.39086345 -0.02783536  0.06598627]]\n",
      "[[-0.3250738  -0.19535372 -0.02651564 -0.23534729]]\n",
      "[[-0.3289809  -0.39008698 -0.03122258  0.04885526]]\n",
      "[[-0.3367826  -0.19453156 -0.03024548 -0.25351286]]\n",
      "[[-0.34067324 -0.38920888 -0.03531573  0.02947876]]\n",
      "[[-0.34845743 -0.19359875 -0.03472616 -0.27413416]]\n",
      "[[-0.3523294  -0.38820845 -0.04020884  0.00739697]]\n",
      "[[-0.36009356 -0.19253361 -0.0400609  -0.29769626]]\n",
      "[[-0.36394423 -0.38706228 -0.04601483 -0.01791214]]\n",
      "[[-0.37168548 -0.19131166 -0.04637307 -0.32475087]]\n",
      "[[-0.37551174 -0.3857437  -0.05286809 -0.04704512]]\n",
      "[[-0.3832266  -0.58006924 -0.05380899  0.22849998]]\n",
      "[[-0.394828   -0.3842213  -0.04923899 -0.08065899]]\n",
      "[[-0.4025124  -0.5786041  -0.05085217  0.1960914 ]]\n",
      "[[-0.4140845  -0.382793   -0.04693034 -0.11218961]]\n",
      "[[-0.42174035 -0.57721215 -0.04917413  0.16532558]]\n",
      "[[-0.43328458 -0.38142207 -0.04586762 -0.14245597]]\n",
      "[[-0.44091302 -0.5758581  -0.04871674  0.13541104]]\n",
      "[[-0.4524302  -0.38007346 -0.04600852 -0.17223461]]\n",
      "[[-0.46003166 -0.5745077  -0.04945321  0.10558646]]\n",
      "[[-0.47152182 -0.37871325 -0.04734148 -0.20227952]]\n",
      "[[-0.47909608 -0.5731273  -0.05138708  0.07510162]]\n",
      "[[-0.49055862 -0.37730777 -0.04988504 -0.2333411 ]]\n",
      "[[-0.49810478 -0.57168275 -0.05455187  0.04319857]]\n",
      "[[-0.5095385  -0.37582272 -0.05368789 -0.26618475]]\n",
      "[[-0.5170549  -0.57013893 -0.05901159  0.00909296]]\n",
      "[[-0.5284577  -0.37422252 -0.05882973 -0.3016095 ]]\n",
      "[[-0.53594214 -0.5684588  -0.06486192 -0.02804507]]\n",
      "[[-0.5473113  -0.7625935  -0.06542282  0.24348864]]\n",
      "[[-0.5625632  -0.56660104 -0.06055305 -0.06909171]]\n",
      "[[-0.5738952  -0.76080495 -0.06193488  0.2038885 ]]\n",
      "[[-0.58911127 -0.5648545  -0.05785711 -0.10767087]]\n",
      "[[-0.6004084  -0.7591016  -0.06001053  0.16621186]]\n",
      "[[-0.6155904  -0.5631743  -0.05668629 -0.14478263]]\n",
      "[[-0.6268539  -0.75744057 -0.05958194  0.12949185]]\n",
      "[[-0.6420027  -0.56151795 -0.05699211 -0.1813773 ]]\n",
      "[[-0.65323305 -0.75578004 -0.06061965  0.09279609]]\n",
      "[[-0.66834867 -0.55984396 -0.05876373 -0.21837944]]\n",
      "[[-0.6795456  -0.7540788  -0.06313132  0.05520333]]\n",
      "[[-0.6946271  -0.55811113 -0.06202725 -0.2567109 ]]\n",
      "[[-0.7057893  -0.7522952  -0.06716147  0.01578074]]\n",
      "[[-0.72083527 -0.5562775  -0.06684586 -0.29731348]]\n",
      "[[-0.73196083 -0.75038606 -0.07279213 -0.026439  ]]\n",
      "[[-0.7469685  -0.5542998  -0.07332091 -0.3411715 ]]\n",
      "[[-0.7580545  -0.7483061  -0.08014434 -0.07248092]]\n",
      "[[-0.7730206  -0.9421931  -0.08159395  0.19387878]]\n",
      "[[-0.7918645  -0.74600446 -0.07771638 -0.12338705]]\n",
      "[[-0.8067846  -0.939932   -0.08018412  0.14380053]]\n",
      "[[-0.8255832  -0.7437587  -0.07730811 -0.1730628 ]]\n",
      "[[-0.8404584  -0.93769395 -0.08076937  0.09426528]]\n",
      "[[-0.8592123  -0.74151284 -0.07888406 -0.22276744]]\n",
      "[[-0.8740426  -0.93542385 -0.08333941  0.04402672]]\n",
      "[[-0.89275104 -0.73921186 -0.08245888 -0.2737432 ]]\n",
      "[[-0.90753525 -0.9330664  -0.08793374 -0.0081649 ]]\n",
      "[[-0.9261966  -0.7368006  -0.08809704 -0.32724535]]\n",
      "[[-0.94093263 -0.93056506 -0.09464195 -0.06359281]]\n",
      "[[-0.9595439  -1.1242117  -0.0959138   0.19779393]]\n",
      "[[-0.9820281  -0.927858   -0.09195792 -0.12353826]]\n",
      "[[-1.0005853  -1.1215504  -0.09442869  0.13877526]]\n",
      "[[-1.0230163  -0.9252117  -0.09165318 -0.18214148]]\n",
      "[[-1.0415206  -1.1189109  -0.09529601  0.08027934]]\n",
      "[[-1.0638988  -0.92256117 -0.09369043 -0.24088353]]\n",
      "[[-1.08235    -1.1162286  -0.0985081   0.02083882]]\n",
      "[[-1.1046746  -0.919842   -0.09809132 -0.30122745]]\n",
      "[[-1.1230714  -1.113439   -0.10411587 -0.04102053]]\n",
      "[[-1.1453402  -1.3069258  -0.10493628  0.21708323]]\n",
      "[[-1.1714786  -1.1104724  -0.10059461 -0.1067705 ]]\n",
      "[[-1.1936882  -1.3040197  -0.10273002  0.15255778]]\n",
      "[[-1.2197685  -1.1075883  -0.09967887 -0.17068549]]\n",
      "[[-1.2419204  -1.3011527  -0.10309258  0.08896285]]\n",
      "[[-1.2679434  -1.1047158  -0.10131332 -0.23438376]]\n",
      "[[-1.2900376  -1.2982552  -0.106001    0.02470159]]\n",
      "[[-1.3160027  -1.1017854  -0.10550696 -0.29945496]]\n",
      "[[-1.3380384  -1.2952577  -0.11149606 -0.04182189]]\n",
      "[[-1.3639436  -1.4886189  -0.1123325   0.21370699]]\n",
      "[[-1.393716   -1.292085   -0.10805836 -0.11219196]]\n",
      "[[-1.4195577  -1.485506   -0.1103022   0.14453992]]\n",
      "[[-1.4492679 -1.2889915 -0.1074114 -0.1808037]]\n",
      "[[-1.4750477  -1.4824256  -0.11102748  0.07615701]]\n",
      "[[-1.5046961  -1.2859015  -0.10950433 -0.24938904]]\n",
      "[[-1.5304142  -1.4793032  -0.11449212  0.00684586]]\n",
      "[[-1.5600003  -1.6726129  -0.1143552   0.26132476]]\n",
      "[[-1.5934526  -1.47606    -0.10912871 -0.06512605]]\n",
      "[[-1.6229737  -1.6694618  -0.11043122  0.19123083]]\n",
      "[[-1.656363   -1.4729476  -0.10660661 -0.13414586]]\n",
      "[[-1.6858219  -1.6663939  -0.10928953  0.12309315]]\n",
      "[[-1.7191498  -1.4698896  -0.10682766 -0.2019718 ]]\n",
      "[[-1.7485476 -1.6633345 -0.1108671  0.0551937]]\n",
      "[[-1.7818143  -1.4668119  -0.10976323 -0.27030858]]\n",
      "[[-1.8111506 -1.6602103 -0.1151694 -0.0141624]]\n",
      "[[-1.8443547  -1.8535084  -0.11545265  0.2400789 ]]\n",
      "[[-1.8814249  -1.6569426  -0.11065107 -0.08667448]]\n",
      "[[-1.9145638  -1.8503189  -0.11238456  0.1691509 ]]\n",
      "[[-1.9515702  -1.6537827  -0.10900154 -0.15676513]]\n",
      "[[-1.9846457  -1.847189   -0.11213684  0.09963939]]\n",
      "[[-2.0215895  -1.6506534  -0.11014405 -0.22621153]]\n",
      "[[-2.0546026  -1.8440429  -0.11466828  0.0297981 ]]\n",
      "[[-2.0914836  -1.6474793  -0.11407232 -0.29674977]]\n",
      "[[-2.124433   -1.8408059  -0.12000731 -0.04210909]]\n",
      "[[-2.1612492 -2.034021  -0.1208495  0.2104313]]\n",
      "[[-2.2019296  -1.8373969  -0.11664087 -0.11779691]]\n",
      "[[-2.2386775  -2.0306711  -0.11899681  0.13592996]]\n",
      "[[-2.279291   -1.8340635  -0.11627821 -0.19179842]]\n",
      "[[-2.3159723  -2.0273466  -0.12011418  0.06205971]]\n",
      "[[-2.3565192  -1.8307257  -0.11887299 -0.26597553]]\n",
      "[[-2.3931336  -2.0239682  -0.1241925  -0.01302383]]\n",
      "[[-2.433613   -2.2171104  -0.12445297  0.2380394 ]]\n"
     ]
    }
   ],
   "source": [
    "cartpole()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f36ebc6",
   "metadata": {},
   "source": [
    "### Реализация 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "984bf9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "087d7caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(2)\n",
      "State space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "RANDOM_SEED = 5\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"Action Space: {}\".format(env.action_space))\n",
    "print(\"State space: {}\".format(env.observation_space))\n",
    "\n",
    "# An episode a full game\n",
    "train_episodes = 300\n",
    "test_episodes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b17a6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state_shape, action_shape):\n",
    "    \"\"\" The agent maps X-states to Y-actions\n",
    "    e.g. The neural network output is [.1, .7, .1, .3]\n",
    "    The highest value 0.7 is the Q-Value.\n",
    "    The index of the highest action (0.7) is action #1.\n",
    "    \"\"\"\n",
    "    learning_rate = 0.001\n",
    "    init = tf.keras.initializers.HeUniform()\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(24, input_shape=state_shape, activation='relu', kernel_initializer=init))\n",
    "    model.add(keras.layers.Dense(12, activation='relu', kernel_initializer=init))\n",
    "    model.add(keras.layers.Dense(action_shape, activation='linear', kernel_initializer=init))\n",
    "    model.compile(loss=tf.keras.losses.Huber(), optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def get_qs(model, state, step):\n",
    "    return model.predict(state.reshape([1, state.shape[0]]))[0]\n",
    "\n",
    "def train(env, replay_memory, model, target_model, done):\n",
    "    learning_rate = 0.7\n",
    "    discount_factor = 0.618\n",
    "\n",
    "    MIN_REPLAY_SIZE = 1000\n",
    "    if len(replay_memory) < MIN_REPLAY_SIZE:\n",
    "        return\n",
    "\n",
    "    batch_size = 64 * 2\n",
    "    mini_batch = random.sample(replay_memory, batch_size)\n",
    "    current_states = np.array([transition[0] for transition in mini_batch])\n",
    "    current_qs_list = model.predict(current_states)\n",
    "    new_current_states = np.array([transition[3] for transition in mini_batch])\n",
    "    future_qs_list = target_model.predict(new_current_states)\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    for index, (observation, action, reward, new_observation, done) in enumerate(mini_batch):\n",
    "        if not done:\n",
    "            max_future_q = reward + discount_factor * np.max(future_qs_list[index])\n",
    "        else:\n",
    "            max_future_q = reward\n",
    "\n",
    "        current_qs = current_qs_list[index]\n",
    "        current_qs[action] = (1 - learning_rate) * current_qs[action] + learning_rate * max_future_q\n",
    "\n",
    "        X.append(observation)\n",
    "        Y.append(current_qs)\n",
    "    model.fit(np.array(X), np.array(Y), batch_size=batch_size, verbose=0, shuffle=True)\n",
    "    \n",
    "def play(model):\n",
    "    env = gym.make('CartPole-v1')\n",
    "    observation = env.reset()\n",
    "    acc_reward = 0;\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        \n",
    "        encoded = observation\n",
    "        encoded_reshaped = encoded.reshape([1, encoded.shape[0]])\n",
    "        predicted = model.predict(encoded_reshaped).flatten()\n",
    "        action = np.argmax(predicted)        \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        acc_reward += reward\n",
    "        \n",
    "    env.close()    \n",
    "    return acc_reward    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8393ea9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training rewards: 19.0 after n steps = 0 with final reward = 1.0\n",
      "Total training rewards: 13.0 after n steps = 1 with final reward = 1.0\n",
      "Total training rewards: 11.0 after n steps = 2 with final reward = 1.0\n",
      "Total training rewards: 14.0 after n steps = 3 with final reward = 1.0\n",
      "Total training rewards: 25.0 after n steps = 4 with final reward = 1.0\n",
      "Total training rewards: 20.0 after n steps = 5 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 20.0 after n steps = 6 with final reward = 1.0\n",
      "Total training rewards: 24.0 after n steps = 7 with final reward = 1.0\n",
      "Total training rewards: 15.0 after n steps = 8 with final reward = 1.0\n",
      "Total training rewards: 17.0 after n steps = 9 with final reward = 1.0\n",
      "Total training rewards: 20.0 after n steps = 10 with final reward = 1.0\n",
      "Total training rewards: 11.0 after n steps = 11 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 34.0 after n steps = 12 with final reward = 1.0\n",
      "Total training rewards: 22.0 after n steps = 13 with final reward = 1.0\n",
      "Total training rewards: 16.0 after n steps = 14 with final reward = 1.0\n",
      "Total training rewards: 18.0 after n steps = 15 with final reward = 1.0\n",
      "Total training rewards: 10.0 after n steps = 16 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 13.0 after n steps = 17 with final reward = 1.0\n",
      "Total training rewards: 24.0 after n steps = 18 with final reward = 1.0\n",
      "Total training rewards: 38.0 after n steps = 19 with final reward = 1.0\n",
      "Total training rewards: 24.0 after n steps = 20 with final reward = 1.0\n",
      "Total training rewards: 19.0 after n steps = 21 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 33.0 after n steps = 22 with final reward = 1.0\n",
      "Total training rewards: 24.0 after n steps = 23 with final reward = 1.0\n",
      "Total training rewards: 9.0 after n steps = 24 with final reward = 1.0\n",
      "Total training rewards: 24.0 after n steps = 25 with final reward = 1.0\n",
      "Total training rewards: 10.0 after n steps = 26 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 25.0 after n steps = 27 with final reward = 1.0\n",
      "Total training rewards: 58.0 after n steps = 28 with final reward = 1.0\n",
      "Total training rewards: 10.0 after n steps = 29 with final reward = 1.0\n",
      "Total training rewards: 18.0 after n steps = 30 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 16.0 after n steps = 31 with final reward = 1.0\n",
      "Total training rewards: 13.0 after n steps = 32 with final reward = 1.0\n",
      "Total training rewards: 27.0 after n steps = 33 with final reward = 1.0\n",
      "Total training rewards: 11.0 after n steps = 34 with final reward = 1.0\n",
      "Total training rewards: 15.0 after n steps = 35 with final reward = 1.0\n",
      "Total training rewards: 18.0 after n steps = 36 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 11.0 after n steps = 37 with final reward = 1.0\n",
      "Total training rewards: 26.0 after n steps = 38 with final reward = 1.0\n",
      "Total training rewards: 18.0 after n steps = 39 with final reward = 1.0\n",
      "Total training rewards: 11.0 after n steps = 40 with final reward = 1.0\n",
      "Total training rewards: 10.0 after n steps = 41 with final reward = 1.0\n",
      "Total training rewards: 16.0 after n steps = 42 with final reward = 1.0\n",
      "Total training rewards: 15.0 after n steps = 43 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 15.0 after n steps = 44 with final reward = 1.0\n",
      "Total training rewards: 12.0 after n steps = 45 with final reward = 1.0\n",
      "Total training rewards: 12.0 after n steps = 46 with final reward = 1.0\n",
      "Total training rewards: 16.0 after n steps = 47 with final reward = 1.0\n",
      "Total training rewards: 9.0 after n steps = 48 with final reward = 1.0\n",
      "Total training rewards: 12.0 after n steps = 49 with final reward = 1.0\n",
      "Total training rewards: 19.0 after n steps = 50 with final reward = 1.0\n",
      "Total training rewards: 18.0 after n steps = 51 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 12.0 after n steps = 52 with final reward = 1.0\n",
      "Total training rewards: 21.0 after n steps = 53 with final reward = 1.0\n",
      "Total training rewards: 15.0 after n steps = 54 with final reward = 1.0\n",
      "Total training rewards: 16.0 after n steps = 55 with final reward = 1.0\n",
      "Total training rewards: 13.0 after n steps = 56 with final reward = 1.0\n",
      "Total training rewards: 13.0 after n steps = 57 with final reward = 1.0\n",
      "Total training rewards: 9.0 after n steps = 58 with final reward = 1.0\n",
      "Total training rewards: 12.0 after n steps = 59 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 9.0 after n steps = 60 with final reward = 1.0\n",
      "Total training rewards: 18.0 after n steps = 61 with final reward = 1.0\n",
      "Total training rewards: 12.0 after n steps = 62 with final reward = 1.0\n",
      "Total training rewards: 16.0 after n steps = 63 with final reward = 1.0\n",
      "Total training rewards: 16.0 after n steps = 64 with final reward = 1.0\n",
      "Total training rewards: 15.0 after n steps = 65 with final reward = 1.0\n",
      "Total training rewards: 9.0 after n steps = 66 with final reward = 1.0\n",
      "Total training rewards: 9.0 after n steps = 67 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 13.0 after n steps = 68 with final reward = 1.0\n",
      "Total training rewards: 14.0 after n steps = 69 with final reward = 1.0\n",
      "Total training rewards: 12.0 after n steps = 70 with final reward = 1.0\n",
      "Total training rewards: 11.0 after n steps = 71 with final reward = 1.0\n",
      "Total training rewards: 11.0 after n steps = 72 with final reward = 1.0\n",
      "Total training rewards: 16.0 after n steps = 73 with final reward = 1.0\n",
      "Total training rewards: 11.0 after n steps = 74 with final reward = 1.0\n",
      "Total training rewards: 12.0 after n steps = 75 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 14.0 after n steps = 76 with final reward = 1.0\n",
      "Total training rewards: 18.0 after n steps = 77 with final reward = 1.0\n",
      "Total training rewards: 13.0 after n steps = 78 with final reward = 1.0\n",
      "Total training rewards: 16.0 after n steps = 79 with final reward = 1.0\n",
      "Total training rewards: 11.0 after n steps = 80 with final reward = 1.0\n",
      "Total training rewards: 9.0 after n steps = 81 with final reward = 1.0\n",
      "Total training rewards: 10.0 after n steps = 82 with final reward = 1.0\n",
      "Total training rewards: 17.0 after n steps = 83 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 14.0 after n steps = 84 with final reward = 1.0\n",
      "Total training rewards: 11.0 after n steps = 85 with final reward = 1.0\n",
      "Total training rewards: 11.0 after n steps = 86 with final reward = 1.0\n",
      "Total training rewards: 11.0 after n steps = 87 with final reward = 1.0\n",
      "Total training rewards: 14.0 after n steps = 88 with final reward = 1.0\n",
      "Total training rewards: 15.0 after n steps = 89 with final reward = 1.0\n",
      "Total training rewards: 11.0 after n steps = 90 with final reward = 1.0\n",
      "Total training rewards: 12.0 after n steps = 91 with final reward = 1.0\n",
      "Total training rewards: 14.0 after n steps = 92 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 14.0 after n steps = 93 with final reward = 1.0\n",
      "Total training rewards: 11.0 after n steps = 94 with final reward = 1.0\n",
      "Total training rewards: 14.0 after n steps = 95 with final reward = 1.0\n",
      "Total training rewards: 11.0 after n steps = 96 with final reward = 1.0\n",
      "Total training rewards: 10.0 after n steps = 97 with final reward = 1.0\n",
      "Total training rewards: 12.0 after n steps = 98 with final reward = 1.0\n",
      "Total training rewards: 9.0 after n steps = 99 with final reward = 1.0\n",
      "Total training rewards: 10.0 after n steps = 100 with final reward = 1.0\n",
      "Total training rewards: 10.0 after n steps = 101 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training rewards: 10.0 after n steps = 102 with final reward = 1.0\n",
      "Total training rewards: 11.0 after n steps = 103 with final reward = 1.0\n",
      "Total training rewards: 13.0 after n steps = 104 with final reward = 1.0\n",
      "Total training rewards: 8.0 after n steps = 105 with final reward = 1.0\n",
      "Total training rewards: 12.0 after n steps = 106 with final reward = 1.0\n",
      "Total training rewards: 10.0 after n steps = 107 with final reward = 1.0\n",
      "Total training rewards: 9.0 after n steps = 108 with final reward = 1.0\n",
      "Total training rewards: 11.0 after n steps = 109 with final reward = 1.0\n",
      "Total training rewards: 11.0 after n steps = 110 with final reward = 1.0\n",
      "Total training rewards: 11.0 after n steps = 111 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 9.0 after n steps = 112 with final reward = 1.0\n",
      "Total training rewards: 8.0 after n steps = 113 with final reward = 1.0\n",
      "Total training rewards: 10.0 after n steps = 114 with final reward = 1.0\n",
      "Total training rewards: 15.0 after n steps = 115 with final reward = 1.0\n",
      "Total training rewards: 13.0 after n steps = 116 with final reward = 1.0\n",
      "Total training rewards: 9.0 after n steps = 117 with final reward = 1.0\n",
      "Total training rewards: 9.0 after n steps = 118 with final reward = 1.0\n",
      "Total training rewards: 9.0 after n steps = 119 with final reward = 1.0\n",
      "Total training rewards: 10.0 after n steps = 120 with final reward = 1.0\n",
      "Total training rewards: 10.0 after n steps = 121 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 13.0 after n steps = 122 with final reward = 1.0\n",
      "Total training rewards: 9.0 after n steps = 123 with final reward = 1.0\n",
      "Total training rewards: 10.0 after n steps = 124 with final reward = 1.0\n",
      "Total training rewards: 10.0 after n steps = 125 with final reward = 1.0\n",
      "Total training rewards: 12.0 after n steps = 126 with final reward = 1.0\n",
      "Total training rewards: 13.0 after n steps = 127 with final reward = 1.0\n",
      "Total training rewards: 13.0 after n steps = 128 with final reward = 1.0\n",
      "Total training rewards: 14.0 after n steps = 129 with final reward = 1.0\n",
      "Total training rewards: 10.0 after n steps = 130 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 10.0 after n steps = 131 with final reward = 1.0\n",
      "Total training rewards: 13.0 after n steps = 132 with final reward = 1.0\n",
      "Total training rewards: 14.0 after n steps = 133 with final reward = 1.0\n",
      "Total training rewards: 14.0 after n steps = 134 with final reward = 1.0\n",
      "Total training rewards: 13.0 after n steps = 135 with final reward = 1.0\n",
      "Total training rewards: 12.0 after n steps = 136 with final reward = 1.0\n",
      "Total training rewards: 11.0 after n steps = 137 with final reward = 1.0\n",
      "Total training rewards: 13.0 after n steps = 138 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 11.0 after n steps = 139 with final reward = 1.0\n",
      "Total training rewards: 14.0 after n steps = 140 with final reward = 1.0\n",
      "Total training rewards: 11.0 after n steps = 141 with final reward = 1.0\n",
      "Total training rewards: 13.0 after n steps = 142 with final reward = 1.0\n",
      "Total training rewards: 9.0 after n steps = 143 with final reward = 1.0\n",
      "Total training rewards: 14.0 after n steps = 144 with final reward = 1.0\n",
      "Total training rewards: 14.0 after n steps = 145 with final reward = 1.0\n",
      "Total training rewards: 10.0 after n steps = 146 with final reward = 1.0\n",
      "Total training rewards: 10.0 after n steps = 147 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 10.0 after n steps = 148 with final reward = 1.0\n",
      "Total training rewards: 14.0 after n steps = 149 with final reward = 1.0\n",
      "Total training rewards: 17.0 after n steps = 150 with final reward = 1.0\n",
      "Total training rewards: 10.0 after n steps = 151 with final reward = 1.0\n",
      "Total training rewards: 11.0 after n steps = 152 with final reward = 1.0\n",
      "Total training rewards: 11.0 after n steps = 153 with final reward = 1.0\n",
      "Total training rewards: 16.0 after n steps = 154 with final reward = 1.0\n",
      "Total training rewards: 10.0 after n steps = 155 with final reward = 1.0\n",
      "Total training rewards: 11.0 after n steps = 156 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 10.0 after n steps = 157 with final reward = 1.0\n",
      "Total training rewards: 10.0 after n steps = 158 with final reward = 1.0\n",
      "Total training rewards: 15.0 after n steps = 159 with final reward = 1.0\n",
      "Total training rewards: 13.0 after n steps = 160 with final reward = 1.0\n",
      "Total training rewards: 10.0 after n steps = 161 with final reward = 1.0\n",
      "Total training rewards: 10.0 after n steps = 162 with final reward = 1.0\n",
      "Total training rewards: 12.0 after n steps = 163 with final reward = 1.0\n",
      "Total training rewards: 11.0 after n steps = 164 with final reward = 1.0\n",
      "Total training rewards: 9.0 after n steps = 165 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 20.0 after n steps = 166 with final reward = 1.0\n",
      "Total training rewards: 11.0 after n steps = 167 with final reward = 1.0\n",
      "Total training rewards: 11.0 after n steps = 168 with final reward = 1.0\n",
      "Total training rewards: 13.0 after n steps = 169 with final reward = 1.0\n",
      "Total training rewards: 14.0 after n steps = 170 with final reward = 1.0\n",
      "Total training rewards: 14.0 after n steps = 171 with final reward = 1.0\n",
      "Total training rewards: 11.0 after n steps = 172 with final reward = 1.0\n",
      "Total training rewards: 13.0 after n steps = 173 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 9.0 after n steps = 174 with final reward = 1.0\n",
      "Total training rewards: 11.0 after n steps = 175 with final reward = 1.0\n",
      "Total training rewards: 9.0 after n steps = 176 with final reward = 1.0\n",
      "Total training rewards: 11.0 after n steps = 177 with final reward = 1.0\n",
      "Total training rewards: 10.0 after n steps = 178 with final reward = 1.0\n",
      "Total training rewards: 11.0 after n steps = 179 with final reward = 1.0\n",
      "Total training rewards: 9.0 after n steps = 180 with final reward = 1.0\n",
      "Total training rewards: 9.0 after n steps = 181 with final reward = 1.0\n",
      "Total training rewards: 13.0 after n steps = 182 with final reward = 1.0\n",
      "Total training rewards: 9.0 after n steps = 183 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 9.0 after n steps = 184 with final reward = 1.0\n",
      "Total training rewards: 32.0 after n steps = 185 with final reward = 1.0\n",
      "Total training rewards: 40.0 after n steps = 186 with final reward = 1.0\n",
      "Total training rewards: 29.0 after n steps = 187 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 37.0 after n steps = 188 with final reward = 1.0\n",
      "Total training rewards: 29.0 after n steps = 189 with final reward = 1.0\n",
      "Total training rewards: 26.0 after n steps = 190 with final reward = 1.0\n",
      "Total training rewards: 12.0 after n steps = 191 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 23.0 after n steps = 192 with final reward = 1.0\n",
      "Total training rewards: 23.0 after n steps = 193 with final reward = 1.0\n",
      "Total training rewards: 26.0 after n steps = 194 with final reward = 1.0\n",
      "Total training rewards: 23.0 after n steps = 195 with final reward = 1.0\n",
      "Total training rewards: 12.0 after n steps = 196 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 24.0 after n steps = 197 with final reward = 1.0\n",
      "Total training rewards: 18.0 after n steps = 198 with final reward = 1.0\n",
      "Total training rewards: 22.0 after n steps = 199 with final reward = 1.0\n",
      "Total training rewards: 24.0 after n steps = 200 with final reward = 1.0\n",
      "Total training rewards: 31.0 after n steps = 201 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 34.0 after n steps = 202 with final reward = 1.0\n",
      "Total training rewards: 40.0 after n steps = 203 with final reward = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training rewards: 22.0 after n steps = 204 with final reward = 1.0\n",
      "Total training rewards: 25.0 after n steps = 205 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 30.0 after n steps = 206 with final reward = 1.0\n",
      "Total training rewards: 22.0 after n steps = 207 with final reward = 1.0\n",
      "Total training rewards: 69.0 after n steps = 208 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 30.0 after n steps = 209 with final reward = 1.0\n",
      "Total training rewards: 52.0 after n steps = 210 with final reward = 1.0\n",
      "Total training rewards: 25.0 after n steps = 211 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 31.0 after n steps = 212 with final reward = 1.0\n",
      "Total training rewards: 28.0 after n steps = 213 with final reward = 1.0\n",
      "Total training rewards: 44.0 after n steps = 214 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 16.0 after n steps = 215 with final reward = 1.0\n",
      "Total training rewards: 74.0 after n steps = 216 with final reward = 1.0\n",
      "Total training rewards: 69.0 after n steps = 217 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 17.0 after n steps = 218 with final reward = 1.0\n",
      "Total training rewards: 75.0 after n steps = 219 with final reward = 1.0\n",
      "Total training rewards: 91.0 after n steps = 220 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 19.0 after n steps = 221 with final reward = 1.0\n",
      "Total training rewards: 15.0 after n steps = 222 with final reward = 1.0\n",
      "Total training rewards: 52.0 after n steps = 223 with final reward = 1.0\n",
      "Total training rewards: 29.0 after n steps = 224 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 29.0 after n steps = 225 with final reward = 1.0\n",
      "Total training rewards: 27.0 after n steps = 226 with final reward = 1.0\n",
      "Total training rewards: 55.0 after n steps = 227 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 40.0 after n steps = 228 with final reward = 1.0\n",
      "Total training rewards: 69.0 after n steps = 229 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 66.0 after n steps = 230 with final reward = 1.0\n",
      "Total training rewards: 103.0 after n steps = 231 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 34.0 after n steps = 232 with final reward = 1.0\n",
      "Total training rewards: 26.0 after n steps = 233 with final reward = 1.0\n",
      "Total training rewards: 80.0 after n steps = 234 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 14.0 after n steps = 235 with final reward = 1.0\n",
      "Total training rewards: 40.0 after n steps = 236 with final reward = 1.0\n",
      "Total training rewards: 55.0 after n steps = 237 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 33.0 after n steps = 238 with final reward = 1.0\n",
      "Total training rewards: 27.0 after n steps = 239 with final reward = 1.0\n",
      "Total training rewards: 65.0 after n steps = 240 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 141.0 after n steps = 241 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 40.0 after n steps = 242 with final reward = 1.0\n",
      "Total training rewards: 27.0 after n steps = 243 with final reward = 1.0\n",
      "Total training rewards: 102.0 after n steps = 244 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 171.0 after n steps = 245 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 130.0 after n steps = 246 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 102.0 after n steps = 247 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 139.0 after n steps = 248 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 40.0 after n steps = 249 with final reward = 1.0\n",
      "Total training rewards: 171.0 after n steps = 250 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 139.0 after n steps = 251 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 237.0 after n steps = 252 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 34.0 after n steps = 253 with final reward = 1.0\n",
      "Total training rewards: 38.0 after n steps = 254 with final reward = 1.0\n",
      "Total training rewards: 46.0 after n steps = 255 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 41.0 after n steps = 256 with final reward = 1.0\n",
      "Total training rewards: 44.0 after n steps = 257 with final reward = 1.0\n",
      "Total training rewards: 37.0 after n steps = 258 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 175.0 after n steps = 259 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 65.0 after n steps = 260 with final reward = 1.0\n",
      "Total training rewards: 172.0 after n steps = 261 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 65.0 after n steps = 262 with final reward = 1.0\n",
      "Total training rewards: 95.0 after n steps = 263 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 36.0 after n steps = 264 with final reward = 1.0\n",
      "Total training rewards: 118.0 after n steps = 265 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 106.0 after n steps = 266 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 61.0 after n steps = 267 with final reward = 1.0\n",
      "Total training rewards: 223.0 after n steps = 268 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 219.0 after n steps = 269 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 422.0 after n steps = 270 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 31.0 after n steps = 271 with final reward = 1.0\n",
      "Total training rewards: 47.0 after n steps = 272 with final reward = 1.0\n",
      "Total training rewards: 276.0 after n steps = 273 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 47.0 after n steps = 274 with final reward = 1.0\n",
      "Total training rewards: 33.0 after n steps = 275 with final reward = 1.0\n",
      "Total training rewards: 117.0 after n steps = 276 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 220.0 after n steps = 277 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 10.0 after n steps = 278 with final reward = 1.0\n",
      "Total training rewards: 305.0 after n steps = 279 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 241.0 after n steps = 280 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 103.0 after n steps = 281 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 120.0 after n steps = 282 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 46.0 after n steps = 283 with final reward = 1.0\n",
      "Total training rewards: 188.0 after n steps = 284 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training rewards: 124.0 after n steps = 285 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 44.0 after n steps = 286 with final reward = 1.0\n",
      "Total training rewards: 237.0 after n steps = 287 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 264.0 after n steps = 288 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 81.0 after n steps = 289 with final reward = 1.0\n",
      "Total training rewards: 406.0 after n steps = 290 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 113.0 after n steps = 291 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 97.0 after n steps = 292 with final reward = 1.0\n",
      "Total training rewards: 47.0 after n steps = 293 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 144.0 after n steps = 294 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 182.0 after n steps = 295 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 23.0 after n steps = 296 with final reward = 1.0\n",
      "Total training rewards: 16.0 after n steps = 297 with final reward = 1.0\n",
      "Total training rewards: 295.0 after n steps = 298 with final reward = 1.0\n",
      "Copying main network weights to the target network weights\n",
      "Total training rewards: 79.0 after n steps = 299 with final reward = 1.0\n"
     ]
    }
   ],
   "source": [
    "epsilon = 1 \n",
    "max_epsilon = 1 \n",
    "min_epsilon = 0.01 \n",
    "decay = 0.01\n",
    "\n",
    "model = agent(env.observation_space.shape, env.action_space.n)\n",
    "\n",
    "target_model = agent(env.observation_space.shape, env.action_space.n)\n",
    "target_model.set_weights(model.get_weights())\n",
    "\n",
    "replay_memory = deque(maxlen=50_000)\n",
    "\n",
    "target_update_counter = 0\n",
    "\n",
    "# X = states, y = actions\n",
    "X, y = [], []\n",
    "\n",
    "steps_to_update_target_model = 0\n",
    "\n",
    "for episode in range(train_episodes):\n",
    "    total_training_rewards = 0\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        steps_to_update_target_model += 1\n",
    "        if True:\n",
    "            env.render()\n",
    "\n",
    "        random_number = np.random.rand()\n",
    "        # 2. Explore using the Epsilon Greedy Exploration Strategy\n",
    "        if random_number <= epsilon:\n",
    "            # Explore\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # Exploit best known action\n",
    "            # model dims are (batch, env.observation_space.n)\n",
    "            encoded = observation\n",
    "            encoded_reshaped = encoded.reshape([1, encoded.shape[0]])\n",
    "            predicted = model.predict(encoded_reshaped).flatten()\n",
    "            action = np.argmax(predicted)\n",
    "        \n",
    "        new_observation, reward, done, info = env.step(action)\n",
    "        replay_memory.append([observation, action, reward, new_observation, done])\n",
    "\n",
    "        # 3. Update the Main Network using the Bellman Equation\n",
    "        if steps_to_update_target_model % 4 == 0 or done:\n",
    "            train(env, replay_memory, model, target_model, done)\n",
    "\n",
    "        observation = new_observation\n",
    "        total_training_rewards += reward\n",
    "\n",
    "        if done:\n",
    "            print('Total training rewards: {} after n steps = {}'.format(total_training_rewards, episode))\n",
    "            total_training_rewards += 1\n",
    "\n",
    "            if steps_to_update_target_model >= 100:\n",
    "                print('Copying main network weights to the target network weights')\n",
    "                target_model.set_weights(model.get_weights())\n",
    "                steps_to_update_target_model = 0\n",
    "            break\n",
    "\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * episode)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3de22d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play(target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27b71a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
