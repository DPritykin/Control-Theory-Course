{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6927c649",
   "metadata": {},
   "source": [
    "## Обучение с подкреплением\n",
    "![Img](https://raw.githubusercontent.com/DPritykin/Control-Theory-Course/main/Term%202%20Nonlinear%20Dynamics%20and%20Control/Images/rl.png)\n",
    "\n",
    "<font size=\"3\">Термин <b>подкрепление (reinforcement)</b>  пришёл из поведенческой психологии и обозначает награду или наказание за некоторый получившийся результат, зависящий не только от самих принятых решений, но и внешних, не обязательно подконтрольных, факторов. Под обучением здесь понимается поиск способов достичь желаемого результата методом проб и ошибок (trial and error), то есть попыток решить задачу и использование накопленного опыта для усовершенствования своей стратегии в будущем.</font>\n",
    "\n",
    "<font size=\"3\">В лекции будут введены основные определения и описана формальная постановка задачи. Под желаемым результатом мы далее будем понимать максимизацию некоторой скалярной величины, называемой <b> вознаграждением (reward)</b> . Интеллектуальную сущность (систему/робота/алгоритм), принимающую решения, будем называть <b> агентом (agent)</b> . Агент взаимодействует с миром (world) или средой (environment), которая задаётся зависящим от времени <b> состоянием (state)</b> . Агенту в каждый момент времени в общем случае доступно только некоторое <b> наблюдение (observation)</b>  текущего состояния мира. Сам агент задаёт процедуру выбора <b> действия (action) </b> по доступным наблюдениям; эту процедуру далее будем называть <b> стратегией (policy)</b> . Процесс взаимодействия агента и среды определяется <b> динамикой среды (world dynamics)</b> , включающей правила смены состояний среды во времени и генерации вознаграждения.</font>\n",
    "\n",
    "<font size=\"3\">Буквы $\\mathbf{s}$, $\\mathbf{a}$, $\\mathbf{r}$ зарезервируем для состояний, действий и наград соответственно; буквой t будем обозначать время в процессе взаимодействия</font>\n",
    "\n",
    "Во многом изложение будет следовать конспекту\n",
    "[Sergey Ivanov Reinforcement Learning Textbook. (Конспект по обучению с подкреплением)](https://arxiv.org/abs/2201.09746)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96d35fb",
   "metadata": {},
   "source": [
    "### Марковская динамическая система\n",
    "\n",
    "Пусть заданы:\n",
    "\n",
    "1. Пространство возможных состояний: $s \\in \\mathcal{S}$;\n",
    "    \n",
    "2. Пространство управляющий воздействий (или действий): $a \\in \\mathcal{A}$;\n",
    "    \n",
    "3. Оператор эволюции системы \n",
    "$$\n",
    "    \\begin{equation*}\n",
    "    T: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S'} \\rightarrow [0,1]\n",
    "    \\end{equation*}\n",
    "$$\n",
    "    (определяет вероятность перехода в состояние $s' \\in \\mathcal{S}$ из состояния $s \\in \\mathcal{S}$ при выборе действия $a \\in \\mathcal{A}$: $s'\\gets p(s'|s,a)$);\n",
    "    \n",
    "4. Вознаграждение\n",
    "$$\n",
    "    \\begin{equation*}\n",
    "    r(s, a, s'): \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S'} \\rightarrow \\mathbb{R}\n",
    "    \\end{equation*}\n",
    "$$\n",
    "\n",
    "5. $\\gamma \\in [0,1)$ - степенной показатель для кумулятивной награды $R$:\n",
    "$$\n",
    "    \\begin{equation}\n",
    "         R = \\sum_{t=0}^{N} \\gamma^t r_t\n",
    "    \\end{equation}\n",
    "$$\n",
    "\n",
    "6. Говорят, что система обладает марковским свойством, если\n",
    "\n",
    "$$p(s_{t+1}=s, r_{t+1} = r|s_t, a_t, r_t, s_{t-1}, a_{t-1}, r_{t-1}, \\dots, s_0, a_0) \\equiv p(s_{t+1}=s, r_{t+1} = r|s_t, a_t), \\quad \\forall t, \\forall s \\in \\mathcal{S}, \\forall a \\in \\mathcal{A},$$\n",
    "то есть эволюция системы не зависит только от текущего состояния и действия, и не зависит от предыдущих состояний и действий.\n",
    "\n",
    "Эта терминология применяется для математического описания алгоритмов взаимодействия агента со средой, имеющих целью обучение агента стратегии (то есть алгоритму выбора действий $a$, исходя из текущего состояния $s$):\n",
    "$$\n",
    "\\pi(s, a) = p(a | s).\n",
    "$$\n",
    "\n",
    "Оптимальной стратегией будем считать стратегию $\\pi_{*}$, максимизирующую ожидаемое вознаграждение\n",
    "$$\n",
    "\\pi_{*}(s, a) = \\text{arg} \\max_{\\pi} E[R].\n",
    "$$\n",
    "\n",
    "Агент взаимодействует со средой, выбирая $\\textit{действия}$ и получая по обратной связи состояния и $\\textit{вознаграждения}$. Исходя из вознаграждений он \"понимает\", правильное ли действие он выбрал и таким образом обучается.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7b403f",
   "metadata": {},
   "source": [
    "### V-функция и Q-функция\n",
    "\n",
    "Введём следующие вспомогательные скалярные функции:\n",
    "\n",
    "1. V-функцию (value-function, аналог функции Беллмана в динамическом программировании):\n",
    "$$\n",
    "    \\begin{equation}\n",
    "            V_{\\pi}(s) = E \\left[\\sum_{t=0}^{\\infty} \\gamma^t r_t | \\pi(s),\\, s_0 = s \\right]\n",
    "    \\end{equation}\n",
    "$$\n",
    "(ожидаемое вознаграждение, которое агент получит при старте из состояния $s$, где $E$ - математическое ожидание.\n",
    "        \n",
    "2. Q-функцию (q-function):\n",
    "$$\n",
    "    \\begin{equation}\n",
    "    Q_{\\pi}(s,a) = \\sum_{s'} T(s,a,s')\\left(r(s,a,s') + \\gamma V_{\\pi}(s')\\right)\n",
    "    \\end{equation}\n",
    "$$    \n",
    "(ожидаемое вознаграждение, которое агент получит, если находясь в состоянии $s$, выберет действие $a$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44b4fd4",
   "metadata": {},
   "source": [
    "### Принцип оптимальности Беллмана для value-функций\n",
    "\n",
    "Принцип оптимальности Беллмана (см. лекции по задачам оптимального управления): экстремаль остаётся экстремалью для любой своей точки выбранной в качестве начальных условий.\n",
    "\n",
    "Если агент стремится выбирать наилучшее действие $a$ для любого наперёд заданного состояния $s$, то\n",
    "$$V_{*}(s) = \\max_a Q_{*}(s,a) \\quad \\text{и} \\quad Q_{*}(s,a) = \\max_{\\pi}Q(s,a).$$\n",
    "\n",
    "Для функции $V(s)$ принцип оптимальности Беллмана выглядит так:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\\label{eq:bellmanV}\n",
    "V(s) = \\max_a{ \\sum_{s'} T(s,a,s')(r(s,a,s') + \\gamma V(s'))}\n",
    "\\end{equation}\n",
    "$$\n",
    "(если мы находимся в состоянии s, мы выбираем лучшее действие)\n",
    "\n",
    "Принцип оптимальности Беллмана позволяет получать значения $V(s)$ при переходе от $s'$ к $s$ (от следующего состояния к предыдущему), если известна $V(s')$.\n",
    "\n",
    "Для Q-функции:\n",
    "$$\n",
    "\\begin{equation}\\label{eq:bellmanQ}\n",
    "Q(s,a) = \\sum_{s'} T(s,a,s')(r(s,a,s') + \\gamma \\max_{a'} Q(s',a'))\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b715b86",
   "metadata": {},
   "source": [
    "### Простейшая среда GridWorld\n",
    "\n",
    "![Img](https://raw.githubusercontent.com/DPritykin/Control-Theory-Course/main/Term%202%20Nonlinear%20Dynamics%20and%20Control/Images/gridworld.png)\n",
    "\n",
    "<font size=\"3\">  Агент начинает игру в произвольном свободном положении(на примере слева снизу поля) и заканчивает в ячейке с  +1 (\"сектор приз\") либо -1 (\"сектор штраф\"). \n",
    "    На каждом шагу он может двигаться вертикально или горизонтально, но не может проходить через препятствия или уходить за пределы игрового поля.</font>\n",
    "\n",
    "#### Детерминированная версия:\n",
    "</font> <font size=\"3\">Подразумевается, что действие на каждом шаге детерминировано, то есть если агент выбрал действие \"переместиться в клетку справа\", то именно такое действие и реализуется (нет случайных факторов, которые могут повлиять на выполнение команды).</font>\n",
    "\n",
    "#### Недетерминированная версия\n",
    "</font> <font size=\"3\">Реализация выбранного действия на каждом шаге не детерминирована, то есть агент, принимая решение перейти на соседнюю клеточку (например, наверх), оказывается на ней с вероятностью 0.8, оставшаяся вероятность равномерно распределяется между перпендикулярными направлениями движения (то есть, если агент принял решение пойти наверх, то перпендикулярные направления - вправо-влево).</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb37a08f",
   "metadata": {},
   "source": [
    "## Поле\n",
    "<font size=\"3\">Зададим переменные, определяющие поле для игры</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cc4768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class WorldOptions(object):\n",
    "    pass\n",
    "\n",
    "options = WorldOptions()\n",
    "options.board_rows = 3\n",
    "options.board_cols = 4\n",
    "options.win_state = (2, 3)\n",
    "options.lose_state = (1, 3)\n",
    "options.start_state = (0, 0)\n",
    "options.obstacles = [(1, 1), (1, 2)]\n",
    "options.deterministic = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54e95a2",
   "metadata": {},
   "source": [
    "<font size=\"3\">Класс <b>GridWorld </b> определяет параметры текущего состояния, а также возвращает следущее состояние в зависимости от выбранного действия</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90372dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, world_options):\n",
    "        self.board = np.zeros([world_options.board_rows, world_options.board_cols])\n",
    "        self.obstacles = world_options.obstacles\n",
    "        for obs in self.obstacles:\n",
    "            self.board[obs[0], obs[1]] = -1\n",
    "        self.action_dict = {\"up\": (1, 0),\n",
    "                            \"down\": (-1, 0),\n",
    "                            \"left\": (0, -1),\n",
    "                            \"right\": (0, 1)\n",
    "                            }\n",
    "        self.start_state = world_options.start_state\n",
    "        self.state =  self.start_state\n",
    "        self.is_terminal_state = False\n",
    "        self.win_state = world_options.win_state\n",
    "        self.lose_state = world_options.lose_state\n",
    "        self.deterministic = world_options.deterministic\n",
    "\n",
    "    # Определяет вознаграждение согласно текущему состоянию\n",
    "    def give_reward(self):\n",
    "        if self.state == self.win_state:\n",
    "            return 1\n",
    "        elif self.state == self.lose_state:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # Проверяет, лежит ли состояние state в границах игрового поля\n",
    "    def is_within_bounds(self, state):\n",
    "        return state[0] >= 0 and state[0] < self.board.shape[0] and \\\n",
    "               state[1] >= 0 and state[1] < self.board.shape[1]\n",
    "\n",
    "    # Возвращает валидное следущее состояние в зависимости от выбранного действия action\n",
    "    def next_position(self, action, move=True):\n",
    "        if not self.deterministic:\n",
    "            if action == \"up\":\n",
    "                action_dst = np.random.choice([\"up\", \"left\", \"right\"], p=[0.8, 0.1, 0.1])\n",
    "            if action == \"down\":\n",
    "                action_dst = np.random.choice([\"down\", \"left\", \"right\"], p=[0.8, 0.1, 0.1])\n",
    "            if action == \"left\":\n",
    "                action_dst =  np.random.choice([\"left\", \"up\", \"down\"], p=[0.8, 0.1, 0.1])\n",
    "            if action == \"right\":\n",
    "                action_dst =  np.random.choice([\"right\", \"up\", \"down\"], p=[0.8, 0.1, 0.1])           \n",
    "        else:\n",
    "            action_dst = action\n",
    "            \n",
    "        next_state = tuple(self.state[i] + self.action_dict[action_dst][i] for i in range(2))\n",
    "            \n",
    "        if not self.is_within_bounds(next_state) or next_state in self.obstacles:\n",
    "            next_state = self.state\n",
    "        \n",
    "        if move:\n",
    "            self.state = next_state\n",
    "            \n",
    "            # Определяет, является ли текущее состояние state терминальным\n",
    "            if (self.state == self.win_state) or (self.state == self.lose_state):\n",
    "                self.is_terminal_state = True            \n",
    "            \n",
    "        return next_state\n",
    "    \n",
    "    # возвращает игру в исходное состояние\n",
    "    def reset(self):\n",
    "        self.state = self.start_state\n",
    "        self.is_terminal_state = False\n",
    "\n",
    "    # Выводит игровое поле\n",
    "    def show_board(self):\n",
    "        for row_id in reversed(range(np.size(self.board, 0))):\n",
    "            print('-----------------')\n",
    "            out = '| '\n",
    "            for col_id in range(np.size(self.board, 1)):\n",
    "                if self.board[row_id, col_id] == -1:\n",
    "                    token = 'X'\n",
    "                if self.board[row_id, col_id] == 0:\n",
    "                    token = '0'\n",
    "                if (row_id, col_id) == self.win_state:\n",
    "                    token = 'W'\n",
    "                if (row_id, col_id) == self.lose_state:\n",
    "                    token = 'L'\n",
    "                if (row_id, col_id) == self.start_state:\n",
    "                    token = '*'                    \n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('-----------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f524bc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Agent(ABC):\n",
    "\n",
    "    def __init__(self, env_options):\n",
    "        self.actions = [\"up\", \"down\", \"left\", \"right\"]  # множество допустимых действий\n",
    "\n",
    "        self.learning_rate = 0.2                        # показатель степени для вычисления кумулятивного вознаграждения # learning rate - \\alpha \n",
    "        self.exploration_rate = 0.3                     # exploration rate - вероятность \"попробовать пойти не туда\" \n",
    "        self.reward_discount = 0.9                      # показатель степени для вычисления кумулятивного вознаграждения (gamma)\n",
    "        \n",
    "        self.env = GridWorld(env_options)               # интерфейс взаимодействия со средой                \n",
    "        self.env.show_board()\n",
    "        \n",
    "        self.traj_states = []                           # последовательный набор кортежей состояние-действие, траектория\n",
    "        self.init_state_values()\n",
    "\n",
    "    # абстрактные метод для реализации в классах-наследниках    \n",
    "    @abstractmethod\n",
    "    def init_state_values(self):     # инициализация нулями V-function или Q-function\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def update_state_values(self):   # пересчёт значений V-function или Q-function в соответствии с алгоритмом обучения\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def extract_value(self, action): # чтение текущего значения V-function или Q-function\n",
    "        pass        \n",
    "    \n",
    "    # Выбирает лучшее действие для текущего состояния\n",
    "    def choose_action(self):\n",
    "        max_next_reward = 0\n",
    "        action = \"up\"\n",
    "\n",
    "        # Добавляем элемент случайности, чтобы агент не ходил всегда по одному пути\n",
    "        if np.random.uniform(0, 1) <= self.exploration_rate:\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            for a in self.actions:\n",
    "                next_reward = self.extract_value(a)\n",
    "\n",
    "                if next_reward >= max_next_reward:\n",
    "                    action = a\n",
    "                    max_next_reward = next_reward\n",
    "        return action\n",
    "\n",
    "    # Возвращает следующее состояние, соответствующее действию action\n",
    "    def take_action(self, action):\n",
    "        position = self.env.next_position(action, move=True)\n",
    "        return \n",
    "\n",
    "    # стирает информацию о пройденной агентом траектории и ставит его на стартовую позицию\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        self.traj_states = []        \n",
    "\n",
    "    # В течение нескольких раундов производим обновление значений, соответствующих каждому состоянию\n",
    "    def play(self, roundsCount=10):\n",
    "        round_id = 0\n",
    "\n",
    "        while round_id < roundsCount:\n",
    "            # Конец раунда\n",
    "            if self.env.is_terminal_state: \n",
    "                self.append_state_to_traj(self.env.state, None)\n",
    "                \n",
    "                # обновление state_values в конце раунда (из конца траектории в начао)\n",
    "                self.update_state_values()\n",
    "\n",
    "                self.reset()\n",
    "                round_id += 1\n",
    "            else:\n",
    "                action = self.choose_action()\n",
    "                \n",
    "                prev_state = self.env.state\n",
    "                # Переходим в следущее состояние, выполняя действие\n",
    "                self.take_action(action)\n",
    "                \n",
    "                # Добавляем состояние к текущей траектории\n",
    "                self.append_state_to_traj(prev_state, action)\n",
    "\n",
    "    # Выводит текущие значения для всех состояний\n",
    "    def show_values(self):\n",
    "        for row_id in reversed(range(np.size(self.state_values, 0))):\n",
    "            print('----------------------------------')\n",
    "            out = '| '\n",
    "            for col_id in range(np.size(self.state_values, 1)):\n",
    "                out += str(self.state_values[(row_id, col_id)]).ljust(6) + ' | '\n",
    "            print(out)\n",
    "        print('----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2e3103",
   "metadata": {},
   "source": [
    "## Алгоритм обучения агента (Value-iteration)\n",
    "<font size=\"3\">В результате обучения агент должен уметь для каждого состояния определять лучшее действие. Для этого мы в первую очередь соотнесем каждому состоянию  значение (ожидаемое вознаграждение), используя формулу:</font>\n",
    "### $V(s_t) \\leftarrow (1-\\alpha) \\cdot V(s_t) + \\alpha \\cdot V(s_{t+1}) = V(s_t) + \\alpha \\cdot [V(s_{t+1}) - V(s_t)], \\tag{1}$\n",
    "<font size=\"3\">где $s_t$ - текущее состояние, $s_{t+1}$ - состояние в следующий момент времени, $\\alpha$ - скорость обучения.</font>\n",
    "\n",
    "<font size=\"3\">Имея такое соответствие, агент cможет на каждом шагу выбирать действие в соответствии с его представлениями о максимальном ожидаемом вознаграждении.</font>\n",
    "\n",
    "<font size=\"3\">В начале агент инициализирует все значения нулем, затем он случайным образом перемещается по полю, пока не доберётся до терминального положения. После этого с помощью формулы (1), проходя по траектории от терминального к начальному состоянию, агент обновляет значения V-функции для всех пройденных состояний.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11b64e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent_V(Agent):\n",
    "    def init_state_values(self):\n",
    "        self.state_values = np.zeros(np.shape(self.env.board))\n",
    "        \n",
    "    def append_state_to_traj(self, state, action):\n",
    "        self.traj_states.append(state)\n",
    "        \n",
    "    def update_state_values(self):\n",
    "        reward = self.env.give_reward()\n",
    "        self.state_values[self.env.state] = reward        \n",
    "        \n",
    "        for state in reversed(self.traj_states):\n",
    "            reward = self.state_values[state] + self.learning_rate * (reward - self.state_values[state]) # формула (1)\n",
    "            self.state_values[state] = round(reward, 3)\n",
    "    \n",
    "    def extract_value(self, action):\n",
    "        next_state = self.env.next_position(action, move=False)\n",
    "        return self.state_values[next_state]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08120a0",
   "metadata": {},
   "source": [
    "## Алгортим обучения агента (Q-learning)\n",
    "\n",
    "<font size=\"3\">1. Рассмотрим кортеж $(s, a, r, s')$:</font>\n",
    "\n",
    "<font size=\"3\">2. Если $Q(s, a)$ вычислена на $k$-й итерации, информация $(s, a, r, s')$ позволяет получить оценку Q-функции для текущей траектории:</font>\n",
    "\n",
    "### $\\hat{Q}(s, a) = r(s,a,s') + \\gamma \\cdot \\max_{a'}{Q_k(s',a')} \\tag{2}$\n",
    "\n",
    "<font size=\"3\">3. Будем вычислять новую оценку для Q-функции как взвешенную сумму имеющейся оценки $Q_k(s, a)$ и новой информации $\\hat{Q}(s,a)$:</font>\n",
    "\n",
    "### $Q_{k+1}(s, a) = (1-\\alpha) \\cdot Q_{k}(s, a) + \\alpha \\cdot \\hat{Q}(s,a) =  Q_{k}(s, a) + \\alpha \\cdot (\\hat{Q}(s,a) - Q_{k}(s, a)) \\tag{3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1362aa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent_Q(Agent):\n",
    "    def init_state_values(self):\n",
    "        self.state_values = {}\n",
    "        for row_id in reversed(range(np.size(self.env.board, 0))):\n",
    "            for col_id in range(np.size(self.env.board, 1)):\n",
    "                self.state_values[(row_id, col_id)] = {}\n",
    "                for action in self.actions:\n",
    "                    self.state_values[(row_id, col_id)][action] = 0  # Q value is a dict of dict\n",
    "                    \n",
    "    def append_state_to_traj(self, state, action):\n",
    "        self.traj_states.append([(state), action])\n",
    "                    \n",
    "    def update_state_values(self):\n",
    "        reward = self.env.give_reward()\n",
    "        \n",
    "        for state in reversed(self.traj_states):\n",
    "            if state[1] is None:\n",
    "                for action in self.actions:\n",
    "                    self.state_values[self.env.state][action] = reward                \n",
    "            else:\n",
    "                current_q_value = self.state_values[state[0]][state[1]]\n",
    "                #q_value_hat = self.reward_discount * reward\n",
    "                updated_q_value = current_q_value + self.learning_rate * (q_value_hat - current_q_value) # формула (3)\n",
    "                self.state_values[state[0]][state[1]] = round(updated_q_value, 3)\n",
    "            \n",
    "            q_value_hat = max(self.state_values[state[0]].values()) # формула 2\n",
    "    \n",
    "    def extract_value(self, action):\n",
    "        return self.state_values[self.env.state][action]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dad630",
   "metadata": {},
   "source": [
    "### Value-iteration в детерминированной среде"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f997e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "| 0 | 0 | 0 | W | \n",
      "-----------------\n",
      "| 0 | X | X | L | \n",
      "-----------------\n",
      "| * | 0 | 0 | 0 | \n",
      "-----------------\n",
      "----------------------------------\n",
      "| 0.994  | 0.996  | 0.998  | 1.0    | \n",
      "----------------------------------\n",
      "| 0.992  | 0.0    | 0.0    | -1.0   | \n",
      "----------------------------------\n",
      "| 0.953  | 0.769  | 0.294  | -0.467 | \n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "options.deterministic = True\n",
    "\n",
    "ag = Agent_V(options)\n",
    "ag.play(100)\n",
    "ag.show_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f38cec8",
   "metadata": {},
   "source": [
    "### Value-iteration в среде со случайными возмущениями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d39e9b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "| 0 | 0 | 0 | W | \n",
      "-----------------\n",
      "| 0 | X | X | L | \n",
      "-----------------\n",
      "| * | 0 | 0 | 0 | \n",
      "-----------------\n",
      "----------------------------------\n",
      "| 0.988  | 0.996  | 0.998  | 1.0    | \n",
      "----------------------------------\n",
      "| 0.986  | 0.0    | 0.0    | -1.0   | \n",
      "----------------------------------\n",
      "| 0.966  | 0.93   | 0.859  | -0.187 | \n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "options.deterministic = False\n",
    "\n",
    "ag = Agent_V(options)\n",
    "ag.play(200)\n",
    "ag.show_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a93e304",
   "metadata": {},
   "source": [
    "### Q-learning в среде со случайными возмущениями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "432a9dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "| 0 | 0 | 0 | W | \n",
      "-----------------\n",
      "| 0 | X | X | L | \n",
      "-----------------\n",
      "| * | 0 | 0 | 0 | \n",
      "-----------------\n",
      "resulting Q-values ... \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(2, 0): {'up': 0.9, 'down': 0.627, 'left': 0.481, 'right': 0.994},\n",
       " (2, 1): {'up': 0.872, 'down': 0.711, 'left': 0.757, 'right': 0.996},\n",
       " (2, 2): {'up': 0.756, 'down': 0.617, 'left': 0.348, 'right': 0.998},\n",
       " (2, 3): {'up': 1, 'down': 1, 'left': 1, 'right': 1},\n",
       " (1, 0): {'up': 0.992, 'down': 0.962, 'left': 0.919, 'right': 0.922},\n",
       " (1, 1): {'up': 0, 'down': 0, 'left': 0, 'right': 0},\n",
       " (1, 2): {'up': 0, 'down': 0, 'left': 0, 'right': 0},\n",
       " (1, 3): {'up': -1, 'down': -1, 'left': -1, 'right': -1},\n",
       " (0, 0): {'up': 0.984, 'down': 0.874, 'left': 0.762, 'right': 0.768},\n",
       " (0, 1): {'up': 0.337, 'down': 0.368, 'left': 0.939, 'right': 0.0},\n",
       " (0, 2): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (0, 3): {'up': -0.723, 'down': 0.0, 'left': -0.216, 'right': -0.006}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options.deterministic = False\n",
    "\n",
    "ag = Agent_Q(options)\n",
    "ag.play(100)\n",
    "\n",
    "print(\"resulting Q-values ... \\n\")\n",
    "ag.state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5717fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
