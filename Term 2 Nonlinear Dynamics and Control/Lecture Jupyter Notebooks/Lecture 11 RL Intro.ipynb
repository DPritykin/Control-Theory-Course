{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6927c649",
   "metadata": {},
   "source": [
    "## Обучение с подкреплением\n",
    "![Img](https://raw.githubusercontent.com/DPritykin/Control-Theory-Course/main/Term%202%20Nonlinear%20Dynamics%20and%20Control/Images/rl.png)\n",
    "\n",
    "<font size=\"3\">Термин <b>подкрепление (reinforcement)</b>  пришёл из поведенческой психологии и обозначает награду или наказание за некоторый получившийся результат, зависящий не только от самих принятых решений, но и внешних, не обязательно подконтрольных, факторов. Под обучением здесь понимается поиск способов достичь желаемого результата методом проб и ошибок (trial and error), то есть попыток решить задачу и использование накопленного опыта для усовершенствования своей стратегии в будущем.</font>\n",
    "\n",
    "<font size=\"3\">В лекции будут введены основные определения и описана формальная постановка задачи. Под желаемым результатом мы далее будем понимать максимизацию некоторой скалярной величины, называемой <b> вознаграждением (reward)</b> . Интеллектуальную сущность (систему/робота/алгоритм), принимающую решения, будем называть <b> агентом (agent)</b> . Агент взаимодействует с миром (world) или средой (environment), которая задаётся зависящим от времени <b> состоянием (state)</b> . Агенту в каждый момент времени в общем случае доступно только некоторое <b> наблюдение (observation)</b>  текущего состояния мира. Сам агент задаёт процедуру выбора <b> действия (action) </b> по доступным наблюдениям; эту процедуру далее будем называть <b> стратегией (policy)</b> . Процесс взаимодействия агента и среды определяется <b> динамикой среды (world dynamics)</b> , включающей правила смены состояний среды во времени и генерации вознаграждения.</font>\n",
    "\n",
    "<font size=\"3\">Буквы $\\mathbf{s}$, $\\mathbf{a}$, $\\mathbf{r}$ зарезервируем для состояний, действий и наград соответственно; буквой t будем обозначать время в процессе взаимодействия</font>\n",
    "\n",
    "Во многом изложение будет следовать конспекту\n",
    "[Sergey Ivanov Reinforcement Learning Textbook. (Конспект по обучению с подкреплением)](https://arxiv.org/abs/2201.09746)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96d35fb",
   "metadata": {},
   "source": [
    "### Марковская динамическая система\n",
    "\n",
    "Пусть заданы:\n",
    "\n",
    "1. Пространство возможных состояний: $s \\in \\mathcal{S}$;\n",
    "    \n",
    "2. Пространство управляющий воздействий (или действий): $a \\in \\mathcal{A}$;\n",
    "    \n",
    "3. Оператор эволюции системы \n",
    "$$\n",
    "    \\begin{equation*}\n",
    "    T: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S'} \\rightarrow [0,1]\n",
    "    \\end{equation*}\n",
    "$$\n",
    "    (ставит в соответствие составительную и действительную вероятность перехода в следующее действие $s'\\gets p(s'|s,a)$);\n",
    "    \n",
    "4. Вознаграждение\n",
    "$$\n",
    "    \\begin{equation*}\n",
    "    R: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S'} \\rightarrow \\mathcal{R};\n",
    "    \\end{equation*}\n",
    "$$\n",
    "\n",
    "5. $\\gamma \\in [0,1)$ - степенной показатель для кумулятивной награды:\n",
    "$$\n",
    "    \\begin{equation}\n",
    "         R = \\sum_{t=0}^{N} \\gamma^t r_t\n",
    "    \\end{equation}\n",
    "$$\n",
    "\n",
    "Эта терминология применяется для математического описания алгоритмов взаимодействия агента со средой, имеющих целью обучение агента стратегиям, ведущим к максимальному вознаграждению:\n",
    "$$\n",
    "\\pi(s, a) = p(a | s) = \\text{arg} \\max_{\\pi} E[R]\n",
    "$$\n",
    "\n",
    "Агент взаимодействует со средой, выбирая \\textit{действия} и получая по обратной связи состояния и \\textit{вознаграждения}. Исходя из вознаграждений он \"понимает\", правильное ли действие он выбрал и таким образом обучается.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7b403f",
   "metadata": {},
   "source": [
    "### V-функция и Q-функция\n",
    "\n",
    "Также мы вводим:\n",
    "\n",
    "\n",
    "1. Функцию полезности (V-функцию):\n",
    "$$\n",
    "    \\begin{equation}\n",
    "            V(s) = E \\left (\\sum_{t=0}^{\\infty} \\gamma^t r_t | \\pi(s),\\, s_0 = s \\right)\n",
    "    \\end{equation}\n",
    "$$\n",
    "(ожидаемое вознаграждение, которое мы получим при старте из состояния $s$, где:\n",
    "* \\(\\pi(s)\\) - стратегия принятия решений (вероятность выбрать действие $a$, находясь в состоянии $s$);\n",
    "* E - математическое ожидание.\n",
    "        \n",
    "2. Функцию качества (Q-функцию):\n",
    "$$\n",
    "    \\begin{equation}\n",
    "    Q(s,a) = \\sum_{s'} T(s,a,s')(R(s,a,s') + \\gamma V(s'))\n",
    "    \\end{equation}\n",
    "$$    \n",
    "(ожидаемое вознаграждение, которое мы получим, если в начальном в состоянии $s$, выбрали действие $a$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44b4fd4",
   "metadata": {},
   "source": [
    "### Принцип оптимальности Беллмана для value-функций\n",
    "\n",
    "Для функции $V(s)$ принцип оптимальности Беллмана выглядит так:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\\label{eq:bellmanV}\n",
    "V(s) = \\max{ \\sum_{s'} T(s,a,s')(R(s,a,s') + \\gamma V(s'))}\n",
    "\\end{equation}\n",
    "$$\n",
    "(если мы находимся в состоянии s, мы выбираем лучшее действие)\n",
    "\n",
    "Принцип оптимальности Беллмана позволяет получать значения $V(s)$ при переходе от $s'$ к $s$ (от следующего состояния к предыдущему), если известна $V(s')$.\n",
    "\n",
    "Для Q-функции:\n",
    "$$\n",
    "\\begin{equation}\\label{eq:bellmanQ}\n",
    "Q(s,a) = \\sum_{s'} T(s,a,s')(R(s,a,s') + \\gamma \\max_{a'} Q(s',a'))\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b715b86",
   "metadata": {},
   "source": [
    "### Простейшая среда GridWorld\n",
    "\n",
    "![Img](https://raw.githubusercontent.com/DPritykin/Control-Theory-Course/main/Term%202%20Nonlinear%20Dynamics%20and%20Control/Images/gridworld.png)\n",
    "\n",
    "<font size=\"3\">  Агент начинает игру в произвольном свободном положении(на примере слева снизу поля) и заканчивает в ячейке с  +1 (\"сектор приз\") либо -1 (\"сектор штраф\"). \n",
    "    На каждом шагу он может двигаться вертикально или горизонтально, но не может проходить через препятствия или уходить за пределы игрового поля.</font>\n",
    "\n",
    "#### Детерминированная версия:\n",
    "</font> <font size=\"3\">Подразумевается, что действие на каждом шаге детерминировано, то есть если агент выбрал действие \"переместиться в клетку справа\", то именно такое действие и реализуется (нет случайных факторов, которые могут повлиять на выполнение команды).</font>\n",
    "\n",
    "#### Недетерминированная версия\n",
    "</font> <font size=\"3\">Реализация выбранного действия на каждом шаге не детерминирована, то есть агент, принимая решение перейти на соседнюю клеточку (например, наверх), оказывается на ней с вероятностью 0.8, оставшаяся вероятность равномерно распределяется между перпендикулярными направлениями движения (то есть, если агент принял решение пойти наверх, то перпендикулярные направления - вправо-влево).</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf4a66c",
   "metadata": {},
   "source": [
    "### Пример 1\n",
    "\n",
    "[Реализация игры Grid World с помощью Value-iteration](https://github.com/DPritykin/Control-Theory-Course/blob/main/Term%202%20Nonlinear%20Dynamics%20and%20Control/Homework%20Problems/RL/Grid%20World%20Value-iteration.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d8f4d2",
   "metadata": {},
   "source": [
    "### Пример 2\n",
    "[Реализация игры Grid World с помощью Q-learning](https://github.com/DPritykin/Control-Theory-Course/blob/main/Term%202%20Nonlinear%20Dynamics%20and%20Control/Homework%20Problems/RL/Grid%20World%20Q-learning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d9303f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
